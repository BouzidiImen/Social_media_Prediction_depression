{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import c3d\n",
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from ekphrasis.classes.spellcorrect import SpellCorrector\n",
    "from ekphrasis.classes.segmenter import Segmenter\n",
    "import re\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "import emoji\n",
    "from tqdm import trange \n",
    "import collections, numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading  Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/5384 [00:00<01:01, 87.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has apparently already been downloaded and unpacked.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5384/5384 [00:17<00:00, 301.35it/s]\n",
      "100%|██████████| 6493/6493 [00:30<00:00, 215.99it/s]\n"
     ]
    }
   ],
   "source": [
    "train, validation, test = c3d.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets, train_labels = train.features, train.labels\n",
    "val_tweets, val_labels = validation.features, validation.labels\n",
    "test_tweets, test_labels = test.features, test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({'negative': 3051, 'positive': 3629}),\n",
       " Counter({'positive': 1228, 'negative': 999}),\n",
       " Counter({'positive': 1636, 'negative': 1334}))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collections.Counter(train_labels),collections.Counter(val_labels),collections.Counter(test_labels) # Data is balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The various text preprocessing steps are:\n",
    "\n",
    " * Tokenization\n",
    " * Lower casing\n",
    " * Stop words removal\n",
    " * Stemming\n",
    " * Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization \n",
    "Tokenization is the process of splitting the given text into smaller pieces called tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So since season is starting I'ma start coming to school looking like a homeless person 🙂👌🏼🏊🏻‍♀️\n",
      "['So', 'since', 'season', 'is', 'starting', \"I'ma\", 'start', 'coming', 'to', 'school', 'looking', 'like', 'a', 'homeless', 'person', '🙂👌🏼🏊🏻\\u200d♀️']\n"
     ]
    }
   ],
   "source": [
    "sentence = train_tweets[0]\n",
    "print(sentence)\n",
    "words = word_tokenize(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lower casing\n",
    "Converting a word to lower case (DEPRESSED -> depressed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "so since season is starting i'ma start coming to school looking like a homeless person 🙂👌🏼🏊🏻‍♀️\n"
     ]
    }
   ],
   "source": [
    "sentence = sentence.lower()\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'my', 'being', 'of', 'a', 'for', 'most', 'our', 'these', 'up', 'an', 'ourselves', 'as', 'can', 'under', 'where', 'your', 'her', 'further', 'its', 'or', 'won', 'before', \"you'll\", \"won't\", \"you're\", 'no', 'theirs', 'his', 'such', 'ours', 'any', 'o', 'what', 'was', 'didn', 'be', 'on', 'm', 'only', 'doesn', 'just', 'yours', 'haven', 'through', \"she's\", 'ain', 'were', 'shouldn', 'in', \"should've\", 'because', 'doing', \"it's\", 'not', 'you', 'hadn', 'against', 'she', 's', 'been', 'but', 'has', 'this', \"you'd\", 'is', 'other', 'y', 'below', 'am', 'whom', 'then', 'so', \"hadn't\", 'more', 'with', 'having', 'too', 'hasn', 're', 'weren', \"shouldn't\", 'hers', 'from', 'few', 'until', 'their', 'why', 'off', 'will', 'at', 'down', 'are', \"shan't\", 'he', 'how', 'there', 'themselves', 'nor', 'herself', 'me', 'if', 'the', 'll', \"hasn't\", 'yourselves', 't', 'himself', 'by', 'above', 'aren', \"haven't\", 'now', 'mustn', 'do', 'them', 'some', 'had', 'who', 'when', 'each', \"that'll\", \"don't\", 'don', 'it', 'which', 'have', 'to', 'does', \"wasn't\", 'did', 'mightn', 'about', \"didn't\", \"needn't\", 'here', \"weren't\", \"wouldn't\", 'after', \"aren't\", 'own', \"couldn't\", 'i', 'very', 'couldn', 'him', 'they', 'all', \"mightn't\", \"doesn't\", 'once', 'those', 'd', 've', 'wouldn', 'yourself', 'over', 'we', 'isn', \"you've\", 'between', 'again', 'myself', 'needn', 'into', 'than', 'wasn', 'same', 'ma', 'itself', 'that', 'during', 'out', 'both', 'and', 'shan', \"isn't\", \"mustn't\", 'while', 'should'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that if NLTK stopwords are used than all the negative contractions will be removed which plays a significant role in sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet Tokenizer: ['So', 'since', 'season', 'is', 'starting', \"I'ma\", 'start', 'coming', 'to', 'school', 'looking', 'like', 'a', 'homeless', 'person', '🙂', '👌', '🏼', '🏊', '🏻', '\\u200d', '♀', '️']\n",
      "\n",
      "Social tokenizer: ['so', 'since', 'season', 'is', 'starting', 'i', \"'\", 'ma', 'start', 'coming', 'to', 'school', 'looking', 'like', 'a', 'homeless', 'person', '🙂', '👌', '🏼', '🏊', '🏻', '\\u200d', '♀️']\n"
     ]
    }
   ],
   "source": [
    "sentence = train_tweets[0]\n",
    "tknzr = TweetTokenizer()\n",
    "social_tokenizer = SocialTokenizer(lowercase=True).tokenize\n",
    "\n",
    "print(\"Tweet Tokenizer:\",tknzr.tokenize(sentence))\n",
    "print()\n",
    "print(\"Social tokenizer:\",social_tokenizer(sentence))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming: \n",
    "It is a process of transforming a word to its root form.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So\n",
      "sinc\n",
      "season\n",
      "is\n",
      "start\n",
      "i'ma\n",
      "start\n",
      "come\n",
      "to\n",
      "school\n",
      "look\n",
      "like\n",
      "a\n",
      "homeless\n",
      "person\n",
      "🙂👌🏼🏊🏻‍♀️\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "for word in sentence.split():\n",
    "    print(ps.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization: \n",
    "Lemmatization reduces the words to a word existing in the language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So\n",
      "since\n",
      "season\n",
      "be\n",
      "start\n",
      "I'ma\n",
      "start\n",
      "come\n",
      "to\n",
      "school\n",
      "look\n",
      "like\n",
      "a\n",
      "homeless\n",
      "person\n",
      "🙂👌🏼🏊🏻‍♀️\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "for word in sentence.split():\n",
    "    print(lemmatizer.lemmatize(word, pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = ' '.join(re.sub(\"(\\w+:\\/\\/\\S+)\", \" \", train_tweets[7]).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How can u be friends with ur mans ex — I LOVE MIRNA SHES FAB AF. Plus wtf were not like 7 ...'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessor on Github: \n",
    "### ekphrasis <a href=\"https://github.com/cbaziotis/ekphrasis\"> on github </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Spell Corrector is based on Peter Norvig's spell-corrector. Just like the segmentation algorithm, we utilize word statistics in order to find the most probable candidate. Besides the provided statistics, you can use your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading english - 1grams ...\n",
      "corrected\n"
     ]
    }
   ],
   "source": [
    "sp = SpellCorrector(corpus=\"english\")\n",
    "print(sp.correct(\"korrectud\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet preprocessor adapted pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_delete = ['theirs', 'she', 'of', \n",
    "                  'all', 'her', 'ourselves', 'that', 'some', 'your', \n",
    "                  'what', 'or', 'me',  'now', 'after',\n",
    "                  'until', 'them', 'through', 'who', 'herself', 'he', \n",
    "                   'y', 'each', 'under', 'hers', 'other', 'down', \n",
    "                  'this', 'their', 'as', 'on','few', 'which', 'further', \n",
    "                  'whom', 'its', 'so', 'yourselves', 'because', 'it', 'both', 'in', 'nor', \n",
    "                    'yours', 'yourself', 'before','since', \n",
    "                  'there', 'himself', 'then', \n",
    "                  'him', 'over',  'here',  'an',  'into','next','d','u','r','im','m','have', \n",
    "                  'the', 'again','such', 'myself', 'they', \n",
    "                  'we', 'those', 'between', 'once','even','have'\n",
    "                   'how', 'from',  'ours', 'during','be','ama','r','i','do','but',\n",
    "                  'his', 'against', 'below',  'to', 'about', \n",
    "                   'by', 'i', 'where', 'a', 'very', 'our', 'my', 'for', 'and','ur'\n",
    "                  'while', 'only', 'up', 'these', 'just', 'same','how',\n",
    "                  'you', 'themselves', 'above', 'with',  'than', \n",
    "                  'own', 'out', 'when', 'any', 'too', 'o', 'at']\n",
    "def load_dict_contractions():\n",
    "    \n",
    "    return {\n",
    "        \"ain't\":\"is not\",\"amn't\":\"am not\",\"aren't\":\"are not\",\"can't\":\"cannot\",\"'cause\":\"because\",\"couldn't\":\"could not\",\n",
    "        \"couldn't've\":\"could not have\",\"could've\":\"could have\",\"daren't\":\"dare not\",\"daresn't\":\"dare not\",\"dasn't\":\"dare not\",\"didn't\":\"did not\",\"doesn't\":\"does not\",\n",
    "        \"don't\":\"do not\",\"e'er\":\"ever\",\"em\":\"them\",\"everyone's\":\"everyone is\",\"finna\":\"fixing to\",\n",
    "        \"gimme\":\"give me\",\"gonna\":\"going to\", \"gon't\":\"go not\",\"gotta\":\"got to\",  \"hadn't\":\"had not\", \"hasn't\":\"has not\",\"haven't\":\"have not\",\n",
    "        \"he'd\":\"he would\", \"he'll\":\"he will\",\"he's\":\"he is\", \"he've\":\"he have\",\"how'd\":\"how would\",\n",
    "        \"how'll\":\"how will\", \"how're\":\"how are\",\"how's\":\"how is\", \"I'd\":\"I would\", \"I'll\":\"I will\", \"I'm\":\"I am\",\n",
    "        \"I'm'a\":\"I am about to\",\"I'm'o\":\"I am going to\",\"isn't\":\"is not\",\"it'd\":\"it would\",\n",
    "        \"it'll\":\"it will\", \"it's\":\"it is\",\"I've\":\"I have\",\"kinda\":\"kind of\",\"let's\":\"let us\",\"mayn't\":\"may not\",\n",
    "        \"may've\":\"may have\",\"mightn't\":\"might not\",\"might've\":\"might have\", \"mustn't\":\"must not\",\"mustn't've\":\"must not have\",\n",
    "        \"must've\":\"must have\",\"needn't\":\"need not\", \"ne'er\":\"never\", \"o'\":\"of\", \"o'er\":\"over\",\"ol'\":\"old\",\n",
    "        \"oughtn't\":\"ought not\",\"shalln't\":\"shall not\",\"shan't\":\"shall not\",\"she'd\":\"she would\",\"she'll\":\"she will\",\"she's\":\"she is\",\n",
    "        \"shouldn't\":\"should not\",\"shouldn't've\":\"should not have\",\"should've\":\"should have\",\"somebody's\":\"somebody is\",\n",
    "        \"someone's\":\"someone is\",\"something's\":\"something is\",\"that'd\":\"that would\",\"that'll\":\"that will\",\n",
    "        \"that're\":\"that are\",\"that's\":\"that is\",\"there'd\":\"there would\",\"there'll\":\"there will\", \"there're\":\"there are\",\"there's\":\"there is\", \"these're\":\"these are\",\n",
    "        \"they'd\":\"they would\",\"they'll\":\"they will\",\"they're\":\"they are\", \"they've\":\"they have\",\"this's\":\"this is\",\n",
    "        \"those're\":\"those are\",\"'tis\":\"it is\",\"'twas\":\"it was\",\"wanna\":\"want to\",\"wasn't\":\"was not\",\"we'd\":\"we would\",\n",
    "        \"we'd've\":\"we would have\",\"we'll\":\"we will\",\"we're\":\"we are\",\"weren't\":\"were not\",\"we've\":\"we have\",\n",
    "        \"what'd\":\"what did\",\"what'll\":\"what will\",\"what're\":\"what are\",\"what's\":\"what is\",\n",
    "        \"what've\":\"what have\",\"when's\":\"when is\",\"where'd\":\"where did\",\"where're\":\"where are\",\"where's\":\"where is\",\n",
    "        \"where've\":\"where have\",\"which's\":\"which is\",\"who'd\":\"who would\",\"who'd've\":\"who would have\",\"who'll\":\"who will\",\n",
    "        \"who're\":\"who are\",\"who's\":\"who is\",\"who've\":\"who have\",\"why'd\":\"why did\",\"why're\":\"why are\",\"why's\":\"why is\",\n",
    "        \"won't\":\"will not\",\"wouldn't\":\"would not\",\"would've\":\"would have\",\"y'all\":\"you all\",\"you'd\":\"you would\",\n",
    "        \"you'll\":\"you will\",\"you're\":\"you are\",\"you've\":\"you have\",\"Whatcha\":\"What are you\",\"luv\":\"love\",\"sux\":\"sucks\"\n",
    "        ,\"shes\":\"she is\",\"wtf\":\"what the fuck\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading english - 1grams ...\n"
     ]
    }
   ],
   "source": [
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "               'time', 'url', 'date', 'number'],\n",
    "    # terms that will be annotated\n",
    "    annotate={\"hashtag\", \"allcaps\", \"elongated\",\n",
    "              'emphasis', 'censored'},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "\n",
    "    # corpus from which the word statistics are going to be used\n",
    "    # for word segmentation\n",
    "    segmenter=\"twitter\",\n",
    "    # corpus from which the word statistics are going to be used\n",
    "    # for spell correction\n",
    "    corrector=\"english\",\n",
    "    unpack_hashtags=False,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correction=True,\n",
    "    tokenizer=SocialTokenizer(lowercase=False).tokenize\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet,processor=text_processor):\n",
    "    tweet=emoji.demojize(tweet)\n",
    "    tweet = tweet.lower()\n",
    "    # Replacement of words such as I've to I have \n",
    "    tweet = tweet.replace(\"’\",\"'\")\n",
    "    tweet = tweet.split()\n",
    "    contractions=load_dict_contractions()\n",
    "    tweet = [contractions[word] if word in contractions else word for word in tweet]\n",
    "    tweet = \" \".join(tweet) \n",
    "    tweet=\" \".join(text_processor.pre_process_doc(tweet))\n",
    "\n",
    "    # remove punctuations\n",
    "    tweet = re.sub(u'[{}]'.format('!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^`{|}~'), u'',tweet)\n",
    "    # Lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    s=''\n",
    "    for word in tweet.split():\n",
    "        s=s+\" \"+lemmatizer.lemmatize(word, pos='v')\n",
    "     # unuseful words removal\n",
    "    for w in words_to_delete:\n",
    "        pattern = r'\\b'+w+r'\\b'\n",
    "        s = re.sub(pattern, '', s)\n",
    "    #correct all multiple white spaces to a single white space\n",
    "    tweet = re.sub('[\\s]+', ' ', s)\n",
    "    #Tokenize \n",
    "    #tokenizer=SocialTokenizer(lowercase=True).tokenize\n",
    "    #resultf=tokenizer(s)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets (tweets):\n",
    "    clean_all_tweets=[]\n",
    "    for i in trange(len(tweets)):\n",
    "         clean_all_tweets.append(clean_tweet(tweets[i]))\n",
    "    return(clean_all_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6680/6680 [00:11<00:00, 559.43it/s]\n"
     ]
    }
   ],
   "source": [
    "Clean_train_tweets=clean_tweets(train_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "So since season is starting I'ma start coming to school looking like a homeless person 🙂👌🏼🏊🏻‍♀️\n",
      " season start start come school look like homeless person slightly_smiling_face ok_hand_medium light_skin_tone woman_swimming_light_skin_tone\n",
      "\n",
      "\n",
      "I never liked it anyway https://t.co/7bTrApaFjd\n",
      " never like anyway url\n",
      "\n",
      "\n",
      "Retweeted Aries Spears (@AriesSpears):\n",
      "\n",
      "I think its disgustin &amp; disrespectful some blk folks r so short sighted... https://t.co/RPicbgeLRl\n",
      " retweeted aries spear user think disgustin disrespectful blk folks short sight url\n",
      "\n",
      "\n",
      "@anniecross00 depression is a mental disorder you cannot help... I've been diagnosed like don't even fucking come at me about mental illness\n",
      " user depression mental disorder cannot help diagnose like not fuck come mental illness\n",
      "\n",
      "\n",
      "@EverythinOakley yikes I hate that stuff\n",
      " user yikes hate stuff\n",
      "\n",
      "\n",
      "Bitch I've done meth and I wasn't tweaking as much as y'all are https://t.co/SV0r58R0d8\n",
      " bitch meth not tweak much url\n",
      "\n",
      "\n",
      "I was diagnosed with severe depression when I was 16.\n",
      " diagnose severe depression number\n",
      "\n",
      "\n",
      "How can u be friends with ur mans ex — I LOVE MIRNA SHES FAB AF. Plus wtf were not like 7 ... https://t.co/ktg25qRq3d\n",
      " can friends ur man ex — love mirna fab af plus fuck not like number url\n",
      "\n",
      "\n",
      "@matthaig1 I have been diagnosed with \"severe depression\" but I resist meds because they make me worse.\n",
      " user diagnose severe depression resist meds make worse\n",
      "\n",
      "\n",
      "@SingFoxx I have been diagnosed with depression. And anxiety. I've been to multiple therapists and taken about four different kinds of\n",
      " user diagnose depression anxiety multiple therapists take four different kinds \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print()\n",
    "    print(train_tweets[i])\n",
    "    print(Clean_train_tweets[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Representation: \n",
    "## Word Representation:\n",
    "### One hot encoding:\n",
    "Binary representation of words: \n",
    "\n",
    "&rArr; Depending on the vocabulary: each word get a representation of 1*n vector representation.\n",
    "\n",
    "Negative aspect of one hot encoding :\n",
    " * The number of dimensions increase linearly as we add words (used memory is Large)\n",
    " * Embedding matrix is very sparse mainly made of zeroes \n",
    " * No context of word because every word is treated on its own\n",
    " * No frequency information is present\n",
    "\n",
    "### Bag-of-words: (BoW)\n",
    "Creates a vocabulary of all the tokens occuring in akk tweets, The frequency of the word in each tweet is inserted. \n",
    "&rArr; The number of dimensions increases with obs in dataset\n",
    "Negative aspect of BoW:\n",
    " * Ignores the meaning of words. The same word can be used in multiple places based on the context or nearby words.\n",
    " * Vector can be huge and it ca be costly for both time and computation\n",
    "\n",
    "&rArr; TF-IDF (term frequency-inverse document frequency)=TF * IDF with TF = Number of times term appears in a document/total number of items in the document and IDF= log(Total number of documents/Number of documents with Term in it)\n",
    " * TF-IDF is based on the bag-of-words (BoW) model, therefore it does not capture position in text, semantics, co-occurrences in different documents.\n",
    "\n",
    "## Word Embedding:\n",
    "### Word2vec:\n",
    "Mapping of words into vectors, the words existing in smilar contexts will have similar word embeddings. \n",
    "To generate vectors from words there's two algorithms:\n",
    " * CBoW(Continuous Bag of Words): Predict the target word from a context &rarr; Small corpus\n",
    " * Skip Gram: Predict the context words from a word &rarr; Large corpus\n",
    "\n",
    "Choosing number of dimensions: Extreme accuracy can be obtained with 300D\n",
    "\n",
    "Pros:\n",
    "   * Calculating the semantic similarity between words.\n",
    "   * Can feed it raw text and it output word vectors\n",
    "\n",
    "Cons:\n",
    "   * Words having multiple sense are represented in one vector \n",
    "   * Can’t handle out-of-vocabulary words, have to re-train to add new words.\n",
    "\n",
    "### GloVe:\n",
    "It is an unsupervised learning algorithm for obtaining vector representations for words. It puts emphasis on the importance co-occurences to extract meaning. The idea behind it is that a certain word generally co-occurs more often with one word than another.\n",
    " \n",
    " &rArr; It proves to perform better than Word2vec in the word analogy tasks.\n",
    "\n",
    "### FastText:\n",
    "It treats each word as composed of character ngrams so the vector for a word is made of the sum of this character n grams.\n",
    " Pros:\n",
    " * Generate better word embeddings for rare words.\n",
    " * Handle Out-of-Vocabulary words unlike Word2vec and GloVe.\n",
    "\n",
    "### ELmo:\n",
    "Instead of using a fixed embedding for each word, like models like GloVe do, ELMo looks at the entire sentence before assigning each word in it its embedding.\n",
    " * ELMo word representations are purely character-based, which allows it to treat out-of-vocabulary tokens unseen during training.\n",
    " * Unlike other word embeddings, it generates word vectors on run time.\n",
    " * It gives embedding of anything you put in — characters, words, sentences, paragraphs, but it is built for sentence embeddings in mind.\n",
    "\n",
    "## BERT(Bidirectional Encoder Representations from Transformer): \n",
    "It uses the transformer architecture in addition to a number of different techniques to train the model. \n",
    "It achives state of the art results. \n",
    "Based on this <a href=\"https://arxiv.org/abs/1810.04805\"> article</a>, <a href=\"https://github.com/google-research/bert\">BERT </a> is the word embedding algorithm I'm using.\n",
    "\n",
    "For visualization (<a href='https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1'>link</a>)\n",
    "\n",
    "Tensoflow hub <a href='https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2'>Link</a>\n",
    "\n",
    "BERT for embedding on <a href='https://medium.com/@aieeshashafique/feature-extraction-from-bert-25887ed2152a'> medium</a>\n",
    "\n",
    "Evaluation of embeddings to check <a href='https://arxiv.org/pdf/1801.09536.pdf'> this article</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version:  2.0.0\n",
      "Hub version:  0.8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import bert\n",
    "FullTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "from tensorflow.keras.models import Model \n",
    "import math\n",
    "print(\"TF version: \", tf.__version__)\n",
    "print(\"Hub version: \", hub.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Prepare inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See BERT paper: https://arxiv.org/pdf/1810.04805.pdf\n",
    "# And BERT implementation convert_single_example() at https://github.com/google-research/bert/blob/master/run_classifier.py\n",
    "\n",
    "def get_masks(tokens, max_seq_length):\n",
    "    \"\"\"Mask for padding\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def get_segments(tokens, max_seq_length):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    segments = []\n",
    "    current_segment_id = 0\n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            current_segment_id = 1\n",
    "    return segments + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def get_ids(tokens, tokenizer, max_seq_length):\n",
    "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 128  # maximum length of a sequence after tokenizing\n",
    "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_mask\")\n",
    "segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"segment_ids\")\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\",\n",
    "                            trainable=True) #pretrained \n",
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "\n",
    "#pooled_output: pooled output of the entire sequence with shape [batch_size, hidden_size].\n",
    "#sequence_output: representations of every token in the input sequence with shape [batch_size, max_sequence_length, hidden_size].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=[pooled_output,sequence_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT tokenizer\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Clean_train_tweets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stokens=tokenizer.tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = get_ids(stokens, tokenizer, max_seq_length)\n",
    "input_masks = get_masks(stokens, max_seq_length)\n",
    "input_segments = get_segments(stokens, max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'never', 'like', 'anyway', 'ur', '##l', '[SEP]']\n",
      "[101, 2196, 2066, 4312, 24471, 2140, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(stokens)\n",
    "print(input_ids)\n",
    "print(input_masks)\n",
    "print(input_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_embs,seq_embs= model.predict([[input_ids],[input_masks],[input_segments]])\n",
    "#Embeddings are generated using the pre-trained model previously loaded ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 128, 768)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.04586147,  0.07677846,  0.15028717, ..., -0.3676868 ,\n",
       "          0.25273815,  0.41104478],\n",
       "        [ 0.50704026, -0.07457219,  0.7028382 , ...,  0.06672566,\n",
       "          0.00297276,  0.2975085 ],\n",
       "        [ 0.8509491 , -0.05657218,  1.025556  , ..., -0.27750093,\n",
       "         -0.30000427, -0.07440143],\n",
       "        ...,\n",
       "        [ 0.12525357,  0.2656413 ,  0.7679463 , ...,  0.01556945,\n",
       "          0.07984328,  0.2148341 ],\n",
       "        [ 0.10444136,  0.21031368,  0.7588055 , ..., -0.02725608,\n",
       "          0.08136114,  0.22190559],\n",
       "        [ 0.13000613,  0.18980533,  0.7512319 , ..., -0.07465325,\n",
       "          0.06056323,  0.15698689]]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_embs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
